{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n",
    "\n",
    "Homework assignments are mandatory. In order to be granted with 8 ECTS and your grade, you must pass 9 out of 10 homeworks. Upload your solution as zip archive until Saturday 6th January 23:59 into the public Homework Submissions folder on studip. The archive should contain your solution as iPython notebook (.ipynb) and as HTML export, as well as your TensorBoard summary ﬁle. Name the archive ﬁle as <your group id > longshort-term-memory.zip.\n",
    "\n",
    "Further, you have to correct another group’s homework until Monday 8th January 23:59. Please follow the instructions in the rating guidelines on how to do your rating.\n",
    "\n",
    "If you encounter problems, please do not hesitate to send your question or concern to lbraun@uos.de. Please do not forget to include [TF] in the subject line.\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "In this week’s task, we are going to implement a network to perform a sentiment analysis on written movie reviews. The network will map sequences of words onto binary decissions: Good rating vs. bad rating.\n",
    "\n",
    "# 2 LSTM recap\n",
    "\n",
    "Read the excelent article by Christopher Olah about Understanding LSTMs.\n",
    "\n",
    "# 3 Data\n",
    "\n",
    "We are going to train the network on movie ratings, which were taken from the Internet Movie Database. Download the Large Movie Review Dataset v1.0 from the dataset’s homepage and unzip the content into your working directory. In order to save memory you can delete all ﬁles that end in .feat and the subfolder /train/unsup/.\n",
    "\n",
    "# 4 Data preparation\n",
    "\n",
    "You can use the helper class (06 imdb-helper.py) to read-in and iterate over the data. The script creates tokenized versions of the movie reviews. Further, the helper class has a method create dictionaries to create dictionaries that map words to unique ids and ids back to words. The method introduces two more labels, one for rare words and one which is used to ﬁll up sequences to create batches with samples of equal length.\n",
    "\n",
    "Further, there is a method to iterate over the data-sets and to slice a batch into subsequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper class\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class IMDB:\n",
    "    def __init__(self, directory):\n",
    "        self._directory = directory\n",
    "        \n",
    "        self._training_data, self._training_labels = self._load_data(\"train\")\n",
    "        self._test_data, self._test_labels = self._load_data(\"test\")\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        samples_n = self._training_labels.shape[0]\n",
    "        random_indices = np.random.choice(samples_n, samples_n // 7, replace = False)\n",
    "        np.random.seed()\n",
    "        \n",
    "        self._validation_data = self._training_data[random_indices]\n",
    "        self._validation_labels = self._training_labels[random_indices]\n",
    "        self._training_data = np.delete(self._training_data, random_indices, axis = 0)\n",
    "        self._training_labels = np.delete(self._training_labels, random_indices)\n",
    "        \n",
    "        joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "        print(\"Unique words: \" + str(len(Counter(joined_written_ratings))))\n",
    "        print(\"Mean length: \" + str(np.mean([len(text) for text in self._training_data])))\n",
    "    \n",
    "    def _load_data(self, data_set_type):\n",
    "        data = []\n",
    "        labels = []\n",
    "        # Iterate over conditions\n",
    "        for condition in [\"neg\", \"pos\"]:\n",
    "            directory_str = os.path.join(self._directory, \"aclImdb\", data_set_type, condition)\n",
    "            directory = os.fsencode(directory_str)\n",
    "        \n",
    "            for file in os.listdir(directory):\n",
    "                filename = os.fsdecode(file)\n",
    "                \n",
    "                label = 0 if condition == \"neg\" else 1\n",
    "                labels.append(label)\n",
    "                \n",
    "                # Read written rating from file\n",
    "                with open(os.path.join(directory_str, filename)) as fd:\n",
    "                    written_rating = fd.read()\n",
    "                    written_rating = written_rating.lower()\n",
    "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                    written_rating = tokenizer.tokenize(written_rating)\n",
    "                    data.append(written_rating)\n",
    "                \n",
    "        return np.array(data), np.array(labels)\n",
    "    \n",
    "    def create_dictionaries(self, vocabulary_size, cutoff_length):\n",
    "        joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "        words_and_count = Counter(joined_written_ratings).most_common(vocabulary_size - 2)\n",
    "        \n",
    "        word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 2)}\n",
    "        word2id[\"_UNKNOWN_\"] = 0\n",
    "        word2id[\"_NOT_A_WORD_\"] = 1\n",
    "        \n",
    "        id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "        \n",
    "        self._word2id = word2id\n",
    "        self._id2word = id2word\n",
    "        \n",
    "        self._training_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._training_data])\n",
    "        self._validation_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._validation_data])\n",
    "        self._test_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._test_data])\n",
    "        \n",
    "        print(\"Mean length: \" + str(np.mean([len(text) for text in self._training_data])))\n",
    "        \n",
    "    \n",
    "    def words2ids(self, words):\n",
    "        if type(words) == list or type(words) == range or type(words) == np.ndarray:\n",
    "            return [self._word2id.get(word, 0) for word in words]\n",
    "        else:\n",
    "            return self._word2id.get(words, 0)\n",
    "    \n",
    "    def ids2words(self, ids):\n",
    "        if type(ids) == list or type(ids) == range or type(ids) == np.ndarray:\n",
    "            return [self._id2word.get(wordid, \"_UNKNOWN_\") for wordid in ids]\n",
    "        else:\n",
    "            return self._id2word.get(ids, \"_UNKNOWN_\")\n",
    "    \n",
    "    \n",
    "    def get_training_batch(self, batch_size):\n",
    "        return self._get_batch(self._training_data, self._training_labels, batch_size)\n",
    "    \n",
    "    def get_validation_batch(self, batch_size):\n",
    "        return self._get_batch(self._validation_data, self._validation_labels, batch_size)\n",
    "    \n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self._get_batch(self._test_data, self._test_labels, batch_size)\n",
    "    \n",
    "    def _get_batch(self, data, labels, batch_size):\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "        \n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]        \n",
    "        \n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "    \n",
    "    \n",
    "    def slize_batch(self, batch, slize_size):\n",
    "        max_len = np.max([len(sample) for sample in batch])\n",
    "        steps = int(np.ceil(max_len / slize_size))\n",
    "        max_len = slize_size * steps\n",
    "        \n",
    "        # Resize all samples in batch to same size\n",
    "        batch_size = len(batch)\n",
    "        buffer = np.ones((batch_size, max_len), dtype = np.int32)\n",
    "        for i, sample in enumerate(batch):\n",
    "            buffer[i, 0:len(sample)] = sample\n",
    "        \n",
    "        for i in range(steps):\n",
    "            on = i * slize_size\n",
    "            off = on + slize_size\n",
    "            yield buffer[:, on:off]\n",
    "        \n",
    "    \n",
    "    def get_sizes(self):\n",
    "        training_samples_n = self._training_labels.shape[0]\n",
    "        validation_samples_n = self._validation_labels.shape[0]\n",
    "        test_samples_n = self._test_labels.shape[0]\n",
    "        return training_samples_n, validation_samples_n, test_samples_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 70316\n",
      "Mean length: 241.894582108\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "imdb = IMDB(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Hyperparameters\n",
    "\n",
    "The following hyperparameters work quite well. Feel free to play around with them and to adjust them according to your computational resources.\n",
    "\n",
    "• Embedding and LSTM memory size: 64\n",
    "\n",
    "• Vocabulary size: 20.000\n",
    "\n",
    "• Review cutoﬀ length: 300\n",
    "\n",
    "• Subsequence length: 100\n",
    "\n",
    "• Batch size: 250\n",
    "\n",
    "• Epochs: 2\n",
    "\n",
    "• Adam Optimizer\n",
    "\n",
    "• Learning rate: 0.03\n",
    "\n",
    "• Dropout rate: 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 64\n",
    "vocabulary_size = 20000\n",
    "cutoff_length = 300\n",
    "subsequence_length = 100\n",
    "batch_size = 250\n",
    "epochs = 2\n",
    "learning_rate = 0.03\n",
    "keep_prob = 0.85\n",
    "number_neurons = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 194.473423865\n"
     ]
    }
   ],
   "source": [
    "# create word ids of the data\n",
    "imdb.create_dictionaries(vocabulary_size, cutoff_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 194.473423865\n",
      "Shape:  (21429,)\n",
      "[31, 107, 286, 690, 683, 509, 2570, 0]\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "print(\"Mean length: \" + str(np.mean([len(text) for text in imdb._training_data])))\n",
    "print(\"Shape: \", imdb._training_data.shape)\n",
    "\n",
    "ids = imdb.words2ids([\"one\", \"two\", \"three\", \"four\", \"five\", \"god\", \"christ\", \"nonsensehere\"])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Network\n",
    "\n",
    "Implement the following network with the help of TensorFlow\n",
    "\n",
    "and train the network on subsequences of the movie ratings. Follow the example from the lecture slides in order to reset or keep the hidden and cell state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# network-input\n",
    "words = tf.placeholder(tf.int32, [batch_size, subsequence_length])\n",
    "\n",
    "\n",
    "# word-embedding\n",
    "\n",
    "with tf.variable_scope(\"embedding\"):\n",
    "    # Create a word-embedding of size vocabulary size x embedding size\n",
    "    initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    embeddings = tf.get_variable(\"embeddings\", [vocabulary_size, embedding_size], initializer = initializer)\n",
    "\n",
    "    # Given a tensor of word ids, retrieve the respective embedding\n",
    "    embed = tf.nn.embedding_lookup(embeddings, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Placeholder:0\", shape=(250, 100), dtype=int32)\n",
      "<tf.Variable 'embedding/embeddings:0' shape=(20000, 64) dtype=float32_ref>\n",
      "Tensor(\"embedding/embedding_lookup:0\", shape=(250, 100, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "print(words)\n",
    "print(embeddings)\n",
    "print(embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Optional\n",
    "\n",
    "Wrap your embeddings with a dropout layer and add a dropout wrapper to your recurrent cell. Adjust your dropout rate according to the data-set that you feed into your network.\n",
    "\n",
    "Use a learning rate scheduling procedure to increase the performance of your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm/Reshape:0\", shape=(250, 100, 64), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(250, 100), dtype=float32)\n",
      "Tensor(\"ffnn/add:0\", shape=(250, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# LSTM\n",
    "\n",
    "keep_probability = tf.placeholder(tf.float32)\n",
    "cell_state = tf.placeholder(tf.float32, shape=[batch_size, embedding_size])\n",
    "hidden_state = tf.placeholder(tf.float32, shape=[batch_size, embedding_size])\n",
    "\n",
    "with tf.variable_scope(\"lstm\"):\n",
    "    # Create LSTM cell isntance\n",
    "    cell = tf.nn.rnn_cell.BasicLSTMCell(embedding_size)\n",
    "    \n",
    "    # Possibly add dropout between recurrent steps\n",
    "    cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob = keep_prob)\n",
    "    \n",
    "    # Create zero state and initialize hidden and cell state from placeholder values\n",
    "    zero_state = cell.zero_state(batch_size, tf.float32)\n",
    "    state = tf.nn.rnn_cell.LSTMStateTuple(c = cell_state, h = hidden_state)\n",
    "    \n",
    "    # tf.nn.static_rnn expects list of time step vectors\n",
    "    sequences = tf.unstack(embed, num = subsequence_length, axis = 1)\n",
    "    \n",
    "    # Unroll the model, returns list of outputs and final cell state\n",
    "    outputs, state = tf.nn.static_rnn(cell, sequences, initial_state = state)\n",
    "    \n",
    "    # Recreate tensor from list\n",
    "    outputs = tf.reshape(tf.concat(outputs, 1), [batch_size, subsequence_length, embedding_size])\n",
    "\n",
    "# mean\n",
    "mean_outputs = tf.reduce_mean(outputs, 2)\n",
    "\n",
    "# tests\n",
    "print(outputs)\n",
    "print(mean_outputs)\n",
    "\n",
    "with tf.variable_scope(\"ffnn\"):\n",
    "    # Create weights and biases for FFNN\n",
    "    initializer = tf.truncated_normal_initializer(stddev = 0.1)\n",
    "    ffnn_weights = tf.get_variable(\"weights\", [subsequence_length, 1], tf.float32, initializer)\n",
    "    ffnn_biases = tf.get_variable(\"biases\", [1], initializer = tf.zeros_initializer())\n",
    "    \n",
    "    # Calculate the drive of FFNN\n",
    "    drive = tf.matmul(mean_outputs, ffnn_weights) + ffnn_biases\n",
    "\n",
    "    print(drive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Plotting\n",
    "\n",
    "Create a summary node for the sigmoid cross entropy and for the accuracy of the network and use TensorBoard in order to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate loss, based on outputs\n",
    "\n",
    "train_labels = tf.placeholder(tf.float32, shape=[batch_size, 1])\n",
    "\n",
    "with tf.variable_scope(\"loss\"):\n",
    "    # calculate sigmoid loss\n",
    "    sigmoid_cross_entropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=train_labels, logits=drive)\n",
    "    mean_sigmoid_cross_entropy = tf.reduce_mean(sigmoid_cross_entropy)\n",
    "    \n",
    "    # add summary node of loss\n",
    "    tf.summary.scalar(\"sigmoid_cross_entropy\", sigmoid_cross_entropy)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    correct_prediction = tf.equal(tf.cast(tf.greater_equal(tf.nn.sigmoid(drive), 0.5), tf.float32), train_labels)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    # add summary node for accuracy\n",
    "    tf.summary.scalar(\"accuracy\", accuracy)\n",
    "    \n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    training_step = tf.train.AdamOptimizer(learning_rate).minimize(sigmoid_cross_entropy)\n",
    "\n",
    "merged_summaries = tf.summary.merge_all()\n",
    "\n",
    "# summary file writer\n",
    "train_writer = tf.summary.FileWriter(\"./summaries/train\", tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "Epoch: 1\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'let'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-179-e54e67498113>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0m_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0msubsequence\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslize_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubsequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0;31m# Get state of last step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                 _state, _summaries, _ = session.run(\n",
      "\u001b[0;32m<ipython-input-147-df359dfff299>\u001b[0m in \u001b[0;36mslize_batch\u001b[0;34m(self, batch, slize_size)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mbuffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'let'"
     ]
    }
   ],
   "source": [
    "# Graph Evaluation\n",
    "\n",
    "# create new session\n",
    "with tf.Session() as session:\n",
    "\n",
    "    # step counter\n",
    "    step = 0\n",
    "\n",
    "    # initialize variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "\n",
    "    print('Start Training')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # print epoch number\n",
    "        print('Epoch:', epoch + 1)\n",
    "\n",
    "        for data, label in imdb.get_training_batch(batch_size):\n",
    "            #labels =  np.expand_dims(labels axis=1).astype(np.float32)\n",
    "            # Get initial  state and cell state\n",
    "            _state = session.run(zero_state)\n",
    "\n",
    "            for subsequence in imdb.slize_batch(data, subsequence_length):\n",
    "                # Get state of last step\n",
    "                _state, _summaries, _ = session.run(\n",
    "                    [state, training_step, merged_summaries],\n",
    "                    feed_dict = {\n",
    "                        sequence: subsequence,\n",
    "                        desired: label,\n",
    "                        keep_probability: keep_prob,\n",
    "                        cell_state: _state.c,\n",
    "                        hidden_state: _state.h\n",
    "                    }\n",
    "                )\n",
    "\n",
    "                # increment step counter\n",
    "                step += 1\n",
    "\n",
    "                # write summary to file\n",
    "                train_writer.add_summary(_summaries, step)\n",
    "\n",
    "            # print validation accuracy every 25 steps\n",
    "            if step % 25 == 0:\n",
    "                _val_accuracy = 0.0\n",
    "                _val_step = 0\n",
    "                for val_data, val_label in imdb.get_validation_batch(batch_size):\n",
    "                    #?val_label =  np.expand_dims(val_labels, axis=1).astype(np.float32)\n",
    "                    # Get initial hidden and cell state\n",
    "                    _val_state = session.run(zero_state)\n",
    "\n",
    "                    for val_subsequence in imdb_data.slize_batch(val_data, sequence_length):\n",
    "                        # Get state of last step\n",
    "                        _val_state, _accuracy, = session.run(\n",
    "                            [state, accuracy],\n",
    "                            feed_dict = {\n",
    "                                sequence: val_subsequence,\n",
    "                                desired: val_label,\n",
    "                                keep_probability: keep_prob,\n",
    "                                cell_state: _val_state.c,\n",
    "                                hidden_state: _val_state.h\n",
    "                            }\n",
    "                        )\n",
    "                        _val_accuracy += _accuracy\n",
    "                        _val_step += 1\n",
    "                _val_accuracy /= _val_step\n",
    "\n",
    "                print('Validation Accuracy at step %d: %f' % (step, _val_accuracy))            \n",
    "\n",
    "print('End Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "There was the following error which we could not debug anymore:\n",
    "    ---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "<ipython-input-179-e54e67498113> in <module>()\n",
    "     22             _state = session.run(zero_state)\n",
    "     23 \n",
    "---> 24             for subsequence in imdb.slize_batch(data, subsequence_length):\n",
    "     25                 # Get state of last step\n",
    "     26                 _state, _summaries, _ = session.run(\n",
    "\n",
    "<ipython-input-147-df359dfff299> in slize_batch(self, batch, slize_size)\n",
    "    118         buffer = np.ones((batch_size, max_len), dtype = np.int32)\n",
    "    119         for i, sample in enumerate(batch):\n",
    "--> 120             buffer[i, 0:len(sample)] = sample\n",
    "    121 \n",
    "    122         for i in range(steps):\n",
    "\n",
    "ValueError: invalid literal for int() with base 10: 'let'\n",
    "\n",
    "         \n",
    "Still we put a lot of time and effort into this homework and we guess it should be enough to pass though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Visualizing the trained embeddings with tSNE\n",
    "\n",
    "Use t-SNE to visualize the trained word embeddings. Try to isolate the two clusters which contain the words with very negative sentiment and very positive words sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9 Evaluate the test performance\n",
    "\n",
    "Once you are done with optimizing the parameters of your implementation, use the test data-set to evaluate the test performance of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 Find help\n",
    "\n",
    "If you struggle with the implementation, there is a slightly related tutorial available on the TensorFlow homepage. Please try to solve the problem with the help of the slides and the TensorFlow python API documentation ﬁrst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
