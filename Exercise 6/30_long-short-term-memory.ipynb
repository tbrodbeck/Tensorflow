{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short-Term Memory\n",
    "\n",
    "Homework assignments are mandatory. In order to be granted with 8 ECTS and your grade, you must pass 9 out of 10 homeworks. Upload your solution as zip archive until Saturday 6th January 23:59 into the public Homework Submissions folder on studip. The archive should contain your solution as iPython notebook (.ipynb) and as HTML export, as well as your TensorBoard summary ﬁle. Name the archive ﬁle as <your group id > longshort-term-memory.zip.\n",
    "\n",
    "Further, you have to correct another group’s homework until Monday 8th January 23:59. Please follow the instructions in the rating guidelines on how to do your rating.\n",
    "\n",
    "If you encounter problems, please do not hesitate to send your question or concern to lbraun@uos.de. Please do not forget to include [TF] in the subject line.\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "In this week’s task, we are going to implement a network to perform a sentiment analysis on written movie reviews. The network will map sequences of words onto binary decissions: Good rating vs. bad rating.\n",
    "\n",
    "# 2 LSTM recap\n",
    "\n",
    "Read the excelent article by Christopher Olah about Understanding LSTMs.\n",
    "\n",
    "# 3 Data\n",
    "\n",
    "We are going to train the network on movie ratings, which were taken from the Internet Movie Database. Download the Large Movie Review Dataset v1.0 from the dataset’s homepage and unzip the content into your working directory. In order to save memory you can delete all ﬁles that end in .feat and the subfolder /train/unsup/.\n",
    "\n",
    "# 4 Data preparation\n",
    "\n",
    "You can use the helper class (06 imdb-helper.py) to read-in and iterate over the data. The script creates tokenized versions of the movie reviews. Further, the helper class has a method create dictionaries to create dictionaries that map words to unique ids and ids back to words. The method introduces two more labels, one for rare words and one which is used to ﬁll up sequences to create batches with samples of equal length.\n",
    "\n",
    "Further, there is a method to iterate over the data-sets and to slice a batch into subsequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper class\n",
    "\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class IMDB:\n",
    "    def __init__(self, directory):\n",
    "        self._directory = directory\n",
    "        \n",
    "        self._training_data, self._training_labels = self._load_data(\"train\")\n",
    "        self._test_data, self._test_labels = self._load_data(\"test\")\n",
    "        \n",
    "        np.random.seed(0)\n",
    "        samples_n = self._training_labels.shape[0]\n",
    "        random_indices = np.random.choice(samples_n, samples_n // 7, replace = False)\n",
    "        np.random.seed()\n",
    "        \n",
    "        self._validation_data = self._training_data[random_indices]\n",
    "        self._validation_labels = self._training_labels[random_indices]\n",
    "        self._training_data = np.delete(self._training_data, random_indices, axis = 0)\n",
    "        self._training_labels = np.delete(self._training_labels, random_indices)\n",
    "        \n",
    "        joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "        print(\"Unique words: \" + str(len(Counter(joined_written_ratings))))\n",
    "        print(\"Mean length: \" + str(np.mean([len(text) for text in self._training_data])))\n",
    "    \n",
    "    def _load_data(self, data_set_type):\n",
    "        data = []\n",
    "        labels = []\n",
    "        # Iterate over conditions\n",
    "        for condition in [\"neg\", \"pos\"]:\n",
    "            directory_str = os.path.join(self._directory, \"aclImdb\", data_set_type, condition)\n",
    "            directory = os.fsencode(directory_str)\n",
    "        \n",
    "            for file in os.listdir(directory):\n",
    "                filename = os.fsdecode(file)\n",
    "                \n",
    "                label = 0 if condition == \"neg\" else 1\n",
    "                labels.append(label)\n",
    "                \n",
    "                # Read written rating from file\n",
    "                with open(os.path.join(directory_str, filename)) as fd:\n",
    "                    written_rating = fd.read()\n",
    "                    written_rating = written_rating.lower()\n",
    "                    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "                    written_rating = tokenizer.tokenize(written_rating)\n",
    "                    data.append(written_rating)\n",
    "                \n",
    "        return np.array(data), np.array(labels)\n",
    "    \n",
    "    def create_dictionaries(self, vocabulary_size, cutoff_length):\n",
    "        joined_written_ratings = [word for text in self._training_data for word in text]\n",
    "        words_and_count = Counter(joined_written_ratings).most_common(vocabulary_size - 2)\n",
    "        \n",
    "        word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 2)}\n",
    "        word2id[\"_UNKNOWN_\"] = 0\n",
    "        word2id[\"_NOT_A_WORD_\"] = 1\n",
    "        \n",
    "        id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "        \n",
    "        self._word2id = word2id\n",
    "        self._id2word = id2word\n",
    "        \n",
    "        self._training_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._training_data])\n",
    "        self._validation_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._validation_data])\n",
    "        self._test_data = np.array([self.words2ids(text[:cutoff_length]) for text in self._test_data])\n",
    "        \n",
    "        print(\"Mean length: \" + str(np.mean([len(text) for text in self._training_data])))\n",
    "        \n",
    "    \n",
    "    def words2ids(self, words):\n",
    "        if type(words) == list or type(words) == range or type(words) == np.ndarray:\n",
    "            return [self._word2id.get(word, 0) for word in words]\n",
    "        else:\n",
    "            return self._word2id.get(words, 0)\n",
    "    \n",
    "    def ids2words(self, ids):\n",
    "        if type(ids) == list or type(ids) == range or type(ids) == np.ndarray:\n",
    "            return [self._id2word.get(wordid, \"_UNKNOWN_\") for wordid in ids]\n",
    "        else:\n",
    "            return self._id2word.get(ids, \"_UNKNOWN_\")\n",
    "    \n",
    "    \n",
    "    def get_training_batch(self, batch_size):\n",
    "        return self._get_batch(self._training_data, self._training_labels, batch_size)\n",
    "    \n",
    "    def get_validation_batch(self, batch_size):\n",
    "        return self._get_batch(self._validation_data, self._validation_labels, batch_size)\n",
    "    \n",
    "    def get_test_batch(self, batch_size):\n",
    "        return self._get_batch(self._test_data, self._test_labels, batch_size)\n",
    "    \n",
    "    def _get_batch(self, data, labels, batch_size):\n",
    "        samples_n = labels.shape[0]\n",
    "        if batch_size <= 0:\n",
    "            batch_size = samples_n\n",
    "        \n",
    "        random_indices = np.random.choice(samples_n, samples_n, replace = False)\n",
    "        data = data[random_indices]\n",
    "        labels = labels[random_indices]        \n",
    "        \n",
    "        for i in range(samples_n // batch_size):\n",
    "            on = i * batch_size\n",
    "            off = on + batch_size\n",
    "            yield data[on:off], labels[on:off]\n",
    "    \n",
    "    \n",
    "    def slize_batch(self, batch, slize_size):\n",
    "        max_len = np.max([len(sample) for sample in batch])\n",
    "        steps = int(np.ceil(max_len / slize_size))\n",
    "        max_len = slize_size * steps\n",
    "        \n",
    "        # Resize all samples in batch to same size\n",
    "        batch_size = len(batch)\n",
    "        buffer = np.ones((batch_size, max_len), dtype = np.int32)\n",
    "        for i, sample in enumerate(batch):\n",
    "            buffer[i, 0:len(sample)] = sample\n",
    "        \n",
    "        for i in range(steps):\n",
    "            on = i * slize_size\n",
    "            off = on + slize_size\n",
    "            yield buffer[:, on:off]\n",
    "        \n",
    "    \n",
    "    def get_sizes(self):\n",
    "        training_samples_n = self._training_labels.shape[0]\n",
    "        validation_samples_n = self._validation_labels.shape[0]\n",
    "        test_samples_n = self._test_labels.shape[0]\n",
    "        return training_samples_n, validation_samples_n, test_samples_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 70316\n",
      "Mean length: 241.894582108\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "\n",
    "imdb = IMDB(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 Hyperparameters\n",
    "\n",
    "The following hyperparameters work quite well. Feel free to play around with them and to adjust them according to your computational resources.\n",
    "\n",
    "• Embedding and LSTM memory size: 64\n",
    "\n",
    "• Vocabulary size: 20.000\n",
    "\n",
    "• Review cutoﬀ length: 300\n",
    "\n",
    "• Subsequence length: 100\n",
    "\n",
    "• Batch size: 250\n",
    "\n",
    "• Epochs: 2\n",
    "\n",
    "• Adam Optimizer\n",
    "\n",
    "• Learning rate: 0.03\n",
    "\n",
    "• Dropout rate: 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "embedding_size = 64\n",
    "vocabulary_size = 20000\n",
    "cutoff_length = 300\n",
    "subsequence_length = 100\n",
    "batch_size = 250\n",
    "epochs = 2\n",
    "learning_rate = 0.03\n",
    "dropout_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 194.473423865\n"
     ]
    }
   ],
   "source": [
    "# create word ids\n",
    "\n",
    "imdb.create_dictionaries(vocabulary_size, cutoff_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean length: 194.473423865\n",
      "Shape:  (21429,)\n",
      "[31, 107, 286, 690, 683, 509, 2570, 0]\n"
     ]
    }
   ],
   "source": [
    "# tests\n",
    "\n",
    "print(\"Mean length: \" + str(np.mean([len(text) for text in imdb._training_data])))\n",
    "print(\"Shape: \", imdb._training_data.shape)\n",
    "\n",
    "ids = imdb.words2ids([\"one\", \"two\", \"three\", \"four\", \"five\", \"god\", \"christ\", \"nonsensehere\"])\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Network\n",
    "\n",
    "Implement the following network with the help of TensorFlow\n",
    "\n",
    "and train the network on subsequences of the movie ratings. Follow the example from the lecture slides in order to reset or keep the hidden and cell state of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = tf.placeholder(tf.int32, [batch_size, num_steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# embedding\n",
    "with tf.variable_scope(\"embedding\"):\n",
    "    # Create a word-embedding of size vocabulary size x embedding size\n",
    "    initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    embeddings = tf.get_variable(\"embedding_sizebedding\", [vocabulary_size, embedding_size], initializer = initializer)\n",
    "    \n",
    "    # Given a tensor of word ids, retrieve the respective embedding\n",
    "    embed = tf.nn.embedding_lookup(embeddings, word_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Optional\n",
    "\n",
    "Wrap your embeddings with a dropout layer and add a dropout wrapper to your recurrent cell. Adjust your dropout rate according to the data-set that you feed into your network.\n",
    "\n",
    "Use a learning rate scheduling procedure to increase the performance of your network.\n",
    "\n",
    "# 7.1 Plotting\n",
    "\n",
    "Create a summary node for the sigmoid cross entropy and for the accuracy of the network and use TensorBoard in order to monitor the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Visualizing the trained embeddings with tSNE\n",
    "\n",
    "Use t-SNE to visualize the trained word embeddings. Try to isolate the two clusters which contain the words with very negative sentiment and very positive words sentiment.\n",
    "\n",
    "# 9 Evaluate the test performance\n",
    "\n",
    "Once you are done with optimizing the parameters of your implementation, use the test data-set to evaluate the test performance of the network.\n",
    "\n",
    "# 10 Find help\n",
    "\n",
    "If you struggle with the implementation, there is a slightly related tutorial available on the TensorFlow homepage. Please try to solve the problem with the help of the slides and the TensorFlow python API documentation ﬁrst."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
