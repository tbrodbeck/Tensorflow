{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Policy Gradients\n",
    "\n",
    "Homework assignments are mandatory. In order to be granted with 8 ECTS and your grade, you must pass 9 out of 10 homeworks. Upload your solution as zip archive until Saturday 13th January 23:59 into the public Homework Submissions folder on studip. The archive should contain your solution as iPython notebook (.ipynb) and as HTML export. Name the archive ﬁle as <your group id > policy-gradients.zip.\n",
    "\n",
    "Further, you have to correct another group’s homework until Monday 15th January 23:59. Please follow the instructions in the rating guidelines on how to do your rating.\n",
    "\n",
    "If you encounter problems, please do not hesitate to send your question or concern to lbraun@uos.de. Please do not forget to include [TF] in the subject line.\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "In this week’s task, we are going to implement a network that learns to balance a pole on a cart. The network will learn how to map an observation to an appropriate action.\n",
    "# 2 Data, OpenAI Gym\n",
    "Install the OpenAI Gym pip package (pip instsall gym). Create a CartPolev0 environment and create a loop that can retrieve an observation and selects and executes a random action, until the episode is done. Create a loop around that loop, such that you can run the environment for several episodes. Render the environment every N episodes and keep in mind, that rendering does slow down the learning drastically, since if the process is not visualized, it does not restrict the frame rate and runs as fast as possible.\n",
    "\n",
    "Observation \n",
    "\n",
    "Each observation is a four-dimensional array: [cart position, cart velocity, pole angle, pole velocity at tip].\n",
    "\n",
    "Action \n",
    "\n",
    "space At each time-step, the agent has to decide to either push the cart to the left or the right.\n",
    "\n",
    "Reward \n",
    "\n",
    "The agent receives a +1 reward for each time-step until the episode terminates.\n",
    "\n",
    "Episode Termination\n",
    "\n",
    "The episode terminates, if either the pole angle is more than +/- 12 degree, the cart moved out of the display or if the episode length is greater than 200. Hence, the maximum reward that can be reached is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#### the graph ####\n",
    "x = tf.placeholder(tf.float32, shape = (1,4))\n",
    "\n",
    "# hidden layer\n",
    "w01 = tf.Variable(tf.random_normal(shape = (4,8), stddev = 0.1))\n",
    "b01 = tf.Variable(tf.random_normal(shape = (1,8), stddev = 0.1))\n",
    "l01 = tf.nn.relu(tf.add(tf.matmul(x,w01), b01))\n",
    "\n",
    "# ouput layer\n",
    "w02 = tf.Variable(tf.random_normal(shape = (8,1), stddev = 0.1))\n",
    "b02 = tf.Variable(tf.random_normal(shape = (1,1), stddev = 0.1))\n",
    "p_right = tf.nn.relu(tf.add(tf.matmul(l01,w02), b02))\n",
    "p_left = 1 - p_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create log likelihoods from a probability distribution over actions\n",
    "log_likelihoods = tf.log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
      "Box(4,)\n",
      "Discrete(2)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "# Create new environment instance \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Investigate environment \n",
    "print(env.metadata) \n",
    "print(env.observation_space) \n",
    "print(env.action_space)\n",
    "\n",
    "# Sample a random action \n",
    "action = env.action_space.sample() \n",
    "print(action)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "with tf.session() as sess:\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Reset Game\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Render current game state (slows down learning!)\n",
    "            if epoch%5:\n",
    "                env.render()\n",
    "\n",
    "            # Create an action (i.e. sample from network)\n",
    "            action = env.action_space.sample()\n",
    "            # Execute action and receive the corresponding reward \n",
    "            # and a new observation\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Improve network ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
