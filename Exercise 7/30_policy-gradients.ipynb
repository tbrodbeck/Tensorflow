{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning: Policy Gradients\n",
    "\n",
    "Homework assignments are mandatory. In order to be granted with 8 ECTS and your grade, you must pass 9 out of 10 homeworks. Upload your solution as zip archive until Saturday 13th January 23:59 into the public Homework Submissions folder on studip. The archive should contain your solution as iPython notebook (.ipynb) and as HTML export. Name the archive ﬁle as <your group id > policy-gradients.zip.\n",
    "\n",
    "Further, you have to correct another group’s homework until Monday 15th January 23:59. Please follow the instructions in the rating guidelines on how to do your rating.\n",
    "\n",
    "If you encounter problems, please do not hesitate to send your question or concern to lbraun@uos.de. Please do not forget to include [TF] in the subject line.\n",
    "\n",
    "# 1 Introduction\n",
    "\n",
    "In this week’s task, we are going to implement a network that learns to balance a pole on a cart. The network will learn how to map an observation to an appropriate action.\n",
    "# 2 Data, OpenAI Gym\n",
    "Install the OpenAI Gym pip package (pip instsall gym). Create a CartPolev0 environment and create a loop that can retrieve an observation and selects and executes a random action, until the episode is done. Create a loop around that loop, such that you can run the environment for several episodes. Render the environment every N episodes and keep in mind, that rendering does slow down the learning drastically, since if the process is not visualized, it does not restrict the frame rate and runs as fast as possible.\n",
    "\n",
    "Observation \n",
    "\n",
    "Each observation is a four-dimensional array: [cart position, cart velocity, pole angle, pole velocity at tip].\n",
    "\n",
    "Action \n",
    "\n",
    "space At each time-step, the agent has to decide to either push the cart to the left or the right.\n",
    "\n",
    "Reward \n",
    "\n",
    "The agent receives a +1 reward for each time-step until the episode terminates.\n",
    "\n",
    "Episode Termination\n",
    "\n",
    "The episode terminates, if either the pole angle is more than +/- 12 degree, the cart moved out of the display or if the episode length is greater than 200. Hence, the maximum reward that can be reached is 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_39:0\", shape=(), dtype=int64)\n",
      "Tensor(\"strided_slice_40:0\", shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as  tf\n",
    "\n",
    "#### the graph ####\n",
    "x = tf.placeholder(tf.float32, shape = (1,4))\n",
    "# hidden layer\n",
    "w01 = tf.Variable(tf.random_normal(shape = (4,8), stddev = 0.1))\n",
    "b01 = tf.Variable(tf.random_normal(shape = (1,8), stddev  = 0.1))\n",
    "l01 = tf.nn.relu(tf.add(tf.matmul(x,w01), b01))\n",
    "\n",
    "# output layer\n",
    "w02 = tf.Variable(tf.random_normal(shape = (8,1), stddev = 0.1))\n",
    "b02 = tf.Variable(tf.random_normal(shape = (1,1), stddev  = 0.1))\n",
    "prob01 = tf.nn.sigmoid(tf.add(tf.matmul(l01,w02), b02))\n",
    "prob02 = tf.subtract(1.0, prob01)\n",
    "\n",
    "# Create log likelihoods from a probability distribution over actions\n",
    "log_likelihoods = tf.log(tf.concat([prob01, prob02],1))\n",
    "\n",
    "# sample action from that distribution\n",
    "action = tf.multinomial(log_likelihoods,1)[0][0]\n",
    "print(action)\n",
    "\n",
    "# select value that corresponds to selected action\n",
    "log_likelihood = log_likelihoods[:, tf.to_int32(action)]\n",
    "print(log_likelihood)\n",
    "\n",
    "# calculate gradient with respect to that value\n",
    "\n",
    "####parameters####\n",
    "episodes = 100\n",
    "episode_length = 200\n",
    "learn_rate = 0.1\n",
    "\n",
    "####execution####\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # initialize variables\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # create environment\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    \n",
    "    for episode in np.arange(episodes):\n",
    "        \n",
    "        # Reset Game\n",
    "        observation = env.reset()\n",
    "        \n",
    "        for step in np.arange(episode_length):\n",
    "            \n",
    "            observation = observation[np.newaxis]\n",
    "            #print(observation)\n",
    "            \n",
    "            # Create an action (i.e. sample from network)\n",
    "            _action = session.run(action, feed_dict = {x: observation})\n",
    "            \n",
    "            # Execute action and receive the corresponding reward \n",
    "            # and a new observation\n",
    "            observation, reward, done, _ = env.step(_action)\n",
    "            \n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            # render the visualization\n",
    "            #if episodes%10:\n",
    "                #env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#### the graph ####\n",
    "x = tf.placeholder(tf.float32, shape = (1,4))\n",
    "\n",
    "# hidden layer\n",
    "w01 = tf.Variable(tf.random_normal(shape = (4,8), stddev = 0.1))\n",
    "b01 = tf.Variable(tf.random_normal(shape = (1,8), stddev = 0.1))\n",
    "l01 = tf.nn.relu(tf.add(tf.matmul(x,w01), b01))\n",
    "\n",
    "# ouput layer\n",
    "w02 = tf.Variable(tf.random_normal(shape = (8,1), stddev = 0.1))\n",
    "b02 = tf.Variable(tf.random_normal(shape = (1,1), stddev = 0.1))\n",
    "p_right = tf.nn.relu(tf.add(tf.matmul(l01,w02), b02))\n",
    "p_left = tf.subtract(1.0,p_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice_9:0\", shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# draw and \n",
    "# create log likelihoods from a probability distribution over actions\n",
    "action = tf.multinomial(tf.log(tf.concat([p_right, p_left], 1)), 1)[0][0]\n",
    "print(action)\n",
    "\n",
    "####parameters####\n",
    "epochs = 100\n",
    "episode_length = 200\n",
    "learn_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'render.modes': ['human', 'rgb_array'], 'video.frames_per_second': 50}\n",
      "Box(4,)\n",
      "Discrete(2)\n",
      "1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument 1 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    269\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 270\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    271\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2707\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2796\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[0;32m-> 2797\u001b[0;31m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a int into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-7faf5c4ced9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Create an action (i.e. sample from network)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m#env.step(session.run(action, feed_dict = {x:observation}))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1109\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \"\"\"\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    272\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    273\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument 1 has invalid type <class 'int'>, must be a string or Tensor. (Can not convert a int into a Tensor or Operation.)"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "# Create new environment instance \n",
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "# Investigate environment \n",
    "print(env.metadata) \n",
    "print(env.observation_space) \n",
    "print(env.action_space)\n",
    "\n",
    "# Sample a random action \n",
    "action = env.action_space.sample() \n",
    "print(action)\n",
    "\n",
    "with tf.Session() as session:\n",
    "    \n",
    "    # initialize\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Reset Game\n",
    "        observation = env.reset()\n",
    "        observation = observation[np.newaxis]\n",
    "        #observation = (env.reset())[np.newaxis]\n",
    "        #observation = np.concatenate((np.array([[1]]),[env.reset()]), 1)\n",
    "        #print(observation)\n",
    "        #print(observation.shape)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Render current game state (slows down learning!)\n",
    "            #if epoch%5:\n",
    "                #env.render()\n",
    "\n",
    "            # Create an action (i.e. sample from network)\n",
    "            observation, reward, done, info = env.step(session.run(action, feed_dict = {x: observation}))\n",
    "\n",
    "            #env.step(session.run(action, feed_dict = {x:observation}))\n",
    "            \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Execute action and receive the corresponding reward \n",
    "            # and a new observation\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "        # Improve network ...\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
