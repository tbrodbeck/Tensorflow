{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data preparation\n",
    "\n",
    "You can use the helper class (05 book-helper.py) to read-in the data. The script creates a tokenized version of the book once you create an instance of it. Since the tf.nn.embedding lookup function expects word ids, we need to map the words in the tokenized version of the book to ids. The create dictionaries method does exactly this. You need to pass the size of your vocabulary to the method. The method will then select the most common words and creates a unique id each for them, all other words are mapped to id 0, which is equivalent to ”unknown”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Book:\n",
    "    def __init__(self, file):\n",
    "        with open(file) as fd:\n",
    "            book = fd.read()\n",
    "            book = book.lower()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            book = tokenizer.tokenize(book)\n",
    "            \n",
    "        print(\"Unique words: \" + str(len(Counter(book))))\n",
    "        self._book_text = book\n",
    "        \n",
    "    \n",
    "    def create_dictionaries(self, vocabulary_size):\n",
    "        words_and_count = Counter(self._book_text).most_common(vocabulary_size - 1)\n",
    "        \n",
    "        word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
    "        word2id[\"UNKNOWN\"] = 0\n",
    "        \n",
    "        id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "        \n",
    "        # Map words to ids\n",
    "        self._book = [word2id.get(word, 0) for word in self._book_text]\n",
    "        \n",
    "        self._word2id = word2id\n",
    "        self._id2word = id2word\n",
    "    \n",
    "    \n",
    "    def words2ids(self, words):\n",
    "        if type(words) == list or type(words) == range or type(words) == np.ndarray:\n",
    "            return [self._word2id.get(word, 0) for word in words]\n",
    "        else:\n",
    "            return self._word2id.get(words, 0)\n",
    "            \n",
    "    def ids2words(self, ids):\n",
    "        if type(ids) == list or type(ids) == range or type(ids) == np.ndarray:\n",
    "            return [self._id2word.get(wordid, \"UNKNOWN\") for wordid in ids]\n",
    "        else:\n",
    "            return self._id2word.get(ids, 0)\n",
    "    \n",
    "    \n",
    "    def get_training_batch(self, batch_size, skip_window):        \n",
    "        valid_indices = range(skip_window, len(self._book) - (skip_window + 1))\n",
    "        context_range = [x for x in range(-skip_window, skip_window + 1) if x != 0]\n",
    "        wordid_contextid_pairs = [(word_id, word_id + shift) for word_id in valid_indices for shift in context_range]\n",
    "        \n",
    "        np.random.shuffle(wordid_contextid_pairs)\n",
    "        \n",
    "        counter = 0\n",
    "        words = np.zeros((batch_size), dtype = np.int32)\n",
    "        contexts = np.zeros((batch_size, 1), dtype = np.int32)\n",
    "        \n",
    "        for word_index, context_index in wordid_contextid_pairs:\n",
    "            words[counter] = self._book[word_index]\n",
    "            contexts[counter, 0] = self._book[context_index]\n",
    "            counter += 1\n",
    "            \n",
    "            if counter == batch_size:\n",
    "                yield words, contexts\n",
    "                counter = 0\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Test the mapping\n",
    "\n",
    "Test the words2ids and ids2words methods by ﬁrst converting a list of words into ids and then back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 13082\n"
     ]
    }
   ],
   "source": [
    "# importing our book\n",
    "bible = Book(\"pg10.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'two', 'three', 'four', 'five', 'god', 'horse', 'pineapple']\n",
      "creating id of the list\n",
      "[75, 162, 232, 316, 302, 27, 1427, 0]\n",
      "and converting it back to words\n",
      "['one', 'two', 'three', 'four', 'five', 'god', 'horse', 'UNKNOWN']\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 10000\n",
    "bible.create_dictionaries(vocabulary_size)\n",
    "\n",
    "\n",
    "list1 = [\"one\", \"two\", \"three\", \"four\", \"five\", \"god\", \"horse\", \"pineapple\"]\n",
    "print(list1)\n",
    "\n",
    "print(\"creating id of the list\")\n",
    "word_ids = bible.words2ids(list1)\n",
    "print(word_ids)\n",
    "\n",
    "print(\"and converting it back to words\")\n",
    "print(bible.ids2words(word_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Embedding\n",
    "\n",
    "Create the word-embedding according to the slides of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding parameter\n",
    "embedding_size = 128\n",
    "noise_samples_n = 64\n",
    "\n",
    "# training parameter\n",
    "learning_rate = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# embedding\n",
    "with tf.variable_scope(\"embedding\"):\n",
    "    # Create a word-embedding of size vocabulary size x embedding size\n",
    "    initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    embeddings = tf.get_variable(\"embedding_sizebedding\", [vocabulary_size, embedding_size], initializer = initializer)\n",
    "    \n",
    "    # Given a tensor of word ids, retrieve the respective embedding\n",
    "    embed = tf.nn.embedding_lookup(embeddings, word_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7 NCE-Loss\n",
    "\n",
    "Use the NCE-loss function and standard gradient descent to train your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, [None, 1])\n",
    "\n",
    "with tf.variable_scope(\"nce-loss\"):\n",
    "    # Create weights and biases for NCE\n",
    "    initializer = tf.truncated_normal_initializer(stddev = 1.0 / np.sqrt(embedding_size))\n",
    "    nce_weights = tf.get_variable(\"weights\", [vocabulary_size, embedding_size], tf.float32, initializer)\n",
    "    nce_biases = tf.get_variable(\"biases\", [vocabulary_size], initializer = tf.zeros_initializer())\n",
    "\n",
    "    # Define loss\n",
    "    loss = tf.nn.nce_loss(\n",
    "        weights = nce_weights,\n",
    "        biases = nce_biases,\n",
    "        labels = y,\n",
    "        inputs = embed,\n",
    "        num_sampled = noise_samples_n,\n",
    "        num_classes = vocabulary_size\n",
    "    )\n",
    "    loss = tf.reduce_mean(loss)\n",
    "\n",
    "with tf.variable_scope(\"optimizer\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_step = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8 Validation\n",
    "\n",
    "Before the ﬁrst and after each epoch, use the cosine similarity in order to monitor the training proses. Print the 7 nearest neighbours of the following words: 5, make, god, jesus, year, sin, israel, and any other you ﬁnd interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unfortunately we only could do it till here. we put really a lot of affort in it but had too many timely and personally issues to cover all tasks of this week\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
