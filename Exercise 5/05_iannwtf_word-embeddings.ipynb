{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Data preparation\n",
    "\n",
    "You can use the helper class (05 book-helper.py) to read-in the data. The script creates a tokenized version of the book once you create an instance of it. Since the tf.nn.embedding lookup function expects word ids, we need to map the words in the tokenized version of the book to ids. The create dictionaries method does exactly this. You need to pass the size of your vocabulary to the method. The method will then select the most common words and creates a unique id each for them, all other words are mapped to id 0, which is equivalent to ”unknown”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class Book:\n",
    "    def __init__(self, file):\n",
    "        with open(file) as fd:\n",
    "            book = fd.read()\n",
    "            book = book.lower()\n",
    "            tokenizer = RegexpTokenizer(r'\\w+')\n",
    "            book = tokenizer.tokenize(book)\n",
    "            \n",
    "        print(\"Unique words: \" + str(len(Counter(book))))\n",
    "        self._book_text = book\n",
    "        \n",
    "    \n",
    "    def create_dictionaries(self, vocabulary_size):\n",
    "        words_and_count = Counter(self._book_text).most_common(vocabulary_size - 1)\n",
    "        \n",
    "        word2id = {word: word_id for word_id, (word, _) in enumerate(words_and_count, 1)}\n",
    "        word2id[\"UNKNOWN\"] = 0\n",
    "        \n",
    "        id2word = dict(zip(word2id.values(), word2id.keys()))\n",
    "        \n",
    "        # Map words to ids\n",
    "        self._book = [word2id.get(word, 0) for word in self._book_text]\n",
    "        \n",
    "        self._word2id = word2id\n",
    "        self._id2word = id2word\n",
    "    \n",
    "    \n",
    "    def words2ids(self, words):\n",
    "        if type(words) == list or type(words) == range or type(words) == np.ndarray:\n",
    "            return [self._word2id.get(word, 0) for word in words]\n",
    "        else:\n",
    "            return self._word2id.get(words, 0)\n",
    "            \n",
    "    def ids2words(self, ids):\n",
    "        if type(ids) == list or type(ids) == range or type(ids) == np.ndarray:\n",
    "            return [self._id2word.get(wordid, \"UNKNOWN\") for wordid in ids]\n",
    "        else:\n",
    "            return self._id2word.get(ids, 0)\n",
    "    \n",
    "    \n",
    "    def get_training_batch(self, batch_size, skip_window):        \n",
    "        valid_indices = range(skip_window, len(self._book) - (skip_window + 1))\n",
    "        context_range = [x for x in range(-skip_window, skip_window + 1) if x != 0]\n",
    "        wordid_contextid_pairs = [(word_id, word_id + shift) for word_id in valid_indices for shift in context_range]\n",
    "        \n",
    "        np.random.shuffle(wordid_contextid_pairs)\n",
    "        \n",
    "        counter = 0\n",
    "        words = np.zeros((batch_size), dtype = np.int32)\n",
    "        contexts = np.zeros((batch_size, 1), dtype = np.int32)\n",
    "        \n",
    "        for word_index, context_index in wordid_contextid_pairs:\n",
    "            words[counter] = self._book[word_index]\n",
    "            contexts[counter, 0] = self._book[context_index]\n",
    "            counter += 1\n",
    "            \n",
    "            if counter == batch_size:\n",
    "                yield words, contexts\n",
    "                counter = 0\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Test the mapping\n",
    "\n",
    "Test the words2ids and ids2words methods by ﬁrst converting a list of words into ids and then back to words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 13082\n",
      "\n",
      "creating id of the list\n",
      "['one', 'two', 'three', 'four', 'five', 'god', 'christ', 'horse']\n",
      "[75, 162, 232, 316, 302, 27, 205, 0]\n",
      "and converting it back to words\n",
      "['one', 'two', 'three', 'four', 'five', 'god', 'christ', 'UNKNOWN']\n"
     ]
    }
   ],
   "source": [
    "bible = Book(\"pg10.txt\")\n",
    "\n",
    "print()\n",
    "\n",
    "bible.create_dictionaries(500)\n",
    "\n",
    "\n",
    "list1 = [\"one\", \"two\", \"three\", \"four\", \"five\", \"god\", \"christ\", \"horse\"]\n",
    "\n",
    "print(\"creating id of the list\")\n",
    "ids = bible.words2ids(list1)\n",
    "\n",
    "print(list1)\n",
    "print(ids)\n",
    "\n",
    "print(\"and converting it back to words\")\n",
    "print(bible.ids2words(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Embedding\n",
    "\n",
    "Create the word-embedding according to the slides of the lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words: 13082\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
